Математической моделью эксперимента является регрессионная модель:
\begin{equation}\label{eq:regression-common-case}
y = f(x, \theta) + \varepsilon,
\end{equation}
где $y$ -- вектор наблюдаемых значений;\\
$x$ -- вектор факторов или контролируемых параметров;\\
$\theta$ -- неизвестные параметры;\\
$f$ -- известная функция;\\
$\varepsilon$ -- случайная ошибка с $E\{\varepsilon\} = 0$.\\
То есть эксперимент рассматривается как некоторая функция $f$, которая зависит от условий проведения эксперимента -- вектора $x$ и неизвестных параметров модели -- вектора $\theta$, а результат проведения эксперимента есть вектор $y$, при этом присутствует случая ошибка измерений $\varepsilon$. 
Задача экспериментатора -- наиболее точно оценить неизвестные параметры $\theta$, проведя при этом минимальное число экспериментов.

Модель регрессии в случае, когда функция $f$ линейна по $\theta$, имеет вид
\begin{equation}
y = f(x, \theta) + \varepsilon= \theta_1 f_1(x) +...+ \theta_m f_m(x) + \varepsilon = \theta'f(x) + \varepsilon.
\end{equation}
где $y$ -- наблюдаемое значение;\\
$(x_1,...,x_m)=x$ -- вектор факторов или контролируемых параметров;\\
$(\theta_1,...,\theta_m)=\theta$ -- неизвестные параметры;\\
$(f_1(x),...,f_m(x))=f(x)$ -- известные функции.\\
$\varepsilon$ -- случайная ошибка.

Пусть было проведено $n$ наблюдений
\begin{equation}
y_i = \theta' f(x^{(i)}) + \varepsilon_i, i = \overline{1, n}, n \ge m
\end{equation}
где $\varepsilon_i$ -- ошибки наблюдений:
$$E\{\varepsilon_i\} = 0$$
$$E\{\varepsilon_i \varepsilon_j\} = 0, i \ne j$$
$$D\{\varepsilon_i\}=\sigma_i ^2$$

Пусть $\hat{\theta} =T(x^{(1)},..,x^{(n)}) $ -- некоторая оценка параметра $\theta$.\\

\newtheorem{theorem}{Теорема}

\begin{theorem}[о наилучшей линейной оценке неизвестного параметра]
	\begin{equation}
	\hat{\theta} = M^{-1} Y,
	\end{equation}
	где 
	$$M = \sum_{i=1}^{n} \frac{1}{\sigma_i^2}f(x^{i})f'(x^{i}), |M| \ne 0 \text{ - информационная матрица Фишера};$$
	$$Y = \sum_{i=1}^{n}\frac{1}{\sigma_i^2} y_i f(x^{i}).$$
	И дисперсионная матрица оценок $\hat \theta$ равна
	\begin{equation}
		D\{ \hat \theta \} = M^{-1}.
	\end{equation}
\end{theorem}
Доказательство сформулированной теоремы приведено в \cite{fedorov}. Оценки такого вида являются лучшими в классе несмещенных линейных оценок, так как обладают наименьшей дисперсией в своём классе. 


В случае линейной регрессии функция $f(x)$ имеет вид:
\begin{equation}\label{eq:linear-regression-f}
f(x) = \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_m x_m.
\end{equation}
  
Обозначим
\begin{equation}
X = 
\begin{bmatrix}
x_{11}	&	\dots	&	x_{1m}\\
\dots	&	\dots	&	\dots\\
x_{n1}	&	\dots	&	x_{nm}
\end{bmatrix}
\end{equation} 
-- матрица плана эксперимента, \\
\begin{equation}
\varepsilon = (\varepsilon_1,.., \varepsilon_n,) \in R^n, E\{\varepsilon\} = 0, E\{\varepsilon \varepsilon'\}=I_n
\end{equation}
-- вектор независимых ошибок со нулевым математическим ожиданием.

Тогда задача \eqref{eq:regression-common-case} в случае линейной регрессии \eqref{eq:linear-regression-f} в матричной форме имеет вид:
\begin{equation}
y = X \theta + \varepsilon
\end{equation}

Также без ограничения общности для каждого набора $x$ можем считать, что  $x_k \in [-1, 1]$, так как линейным преобразованием отрезок $x_k \in [a_k, b_k]$ можно перевести в $[-1, 1]$:
$$z_k = \frac{x_k - (a_k + b_k)/2}{(b_k - a_k)/2}, k = \overline{1, m}$$ 

\section{План эксперимента}

Экспериментатор заинтересован в проведении эксперимента с наибольшей точностью и минимальными финансовыми и иными затратами. Формализуем понятие точности и затрат.

Обозначим эксперимент буквой $\varepsilon$, а затраты на его проведение -- $\tau$, то есть эксперимент есть $\varepsilon(\tau)$. Введём функцию $R(\varepsilon)$ -- функция потерь эксперимента $\varepsilon(\tau)$. Будем считать, что эксперимент $\varepsilon_1$ лучше, предпочтительнее эксперимента $\varepsilon_2$, если $R(\varepsilon_1) < R(\varepsilon_2)$.\\
Точность оценок вычисляется на основании дисперсионной матрицы оценок $D(\varepsilon)$. Точность задаётся как некоторый функционал $\Psi$ от дисперсионной матрицы оценок: $\Psi[D(\varepsilon)]$.\\
Тогда $R(\varepsilon)$ можно представить в виде:
$$R(\varepsilon) = \tau + \Psi[D(\varepsilon)].$$

\begin{definition}
	Совокупность величин
	\begin{gather}
	x_1, x_2, \dots, x_n;\\
	r_1, r_2, \dots, r_n;\\
	\sum_{i=1}^n r_i = N
	\end{gather}
	называется планом (эксперимента) $\varepsilon(N)$, где $x_1, \dots x_n$ -- точки, в которых проводятся наблюдения, $r_1 \dots r_n$ -- число наблюдений в каждой точке, $N$ -- общее число наблюдений. Совокупность точек  $x_1, x_2, \dots, x_n$ называется спектром плана $\varepsilon(N)$.
\end{definition}

\begin{definition}
	Нормированным планом $\varepsilon(N)$ называется совокупность величин
	\begin{gather}
	x_1, x_2, \dots, x_n;\\
	p_1, p_2, \dots, p_n;\\
	\sum_{i = 1}^n p_i = 1, p_i = \frac {r_i} {N}.
	\end{gather}
\end{definition}

В условиях, когда  $N$ настолько велико, что функцию потерь можно рассматривать как непрерывную по $N$, вводится понятие непрерывного нормированного плана.
\begin{definition}
	Непрерывным нормированным планом называется совокупность величин
	\begin{gather}
	x_1, x_2, \dots, x_n;\\
	\rho_1, \rho_2, \dots, \rho_n;\\
	\sum_{i = 1}^n \rho_i = 1, \rho_i \in (0, 1).
	\end{gather}
\end{definition}

Существуют различные критерии оптимальности плана эксперимента. В данной работе будут рассмотрены $D$-оптимальные планы.

\begin{definition}
	План эксперимента называется $D$-оптимальным, когда модуль определителя дисперсионной матрицы оценок параметров минимален:
	\begin{equation}\label{def:d-opt}
	|\det D\{\hat \theta\} | \rightarrow \min_{x \in X}.
	\end{equation}
\end{definition}

Но определитель обратной матрицы обратно пропорционален определителю исходной матрицы:
\begin{equation*}
\det {D\{\hat \theta\}} = \det {M^{-1}\{\hat \theta\}} = \frac 1 {\det M(x)}.
\end{equation*}
Таким образом задача построения $D$-оптимального плана \ref{def:d-opt} свелась к задаче о максимизации модуля определителя информационной матрицы плана:
\begin{equation}
|det M(x)| \rightarrow \max_{x \in X}.
\end{equation}
 